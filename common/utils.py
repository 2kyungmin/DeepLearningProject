import re
from pathlib import Path
from typing import List
from dataclasses import dataclass
import numpy as np
import sys

def count_utterance_by_speaker(cha_file: str) -> dict:
    """
    Ïã§Ï†ú Î∞úÌôîÌïú ÌôîÏûêÎßå Î∞òÌôò (0 Î∞úÌôî Ï†úÏô∏)
    """
    content = Path(cha_file).read_text(errors='ignore')
    
    # Ïã§Ï†ú *SPEAKER Î∞úÌôîÎßå
    speaker_utts = re.findall(r'\*([A-Z][A-Za-z ]+?):\s*(.*?)(?=\n\*[A-Z][A-Za-z ]+?:|\n%|\n@|\Z)', 
                             content, re.DOTALL | re.I)
    
    active_speakers = {}
    for speaker, text in speaker_utts:
        # ÌÅ¥Î¶¨Îãù ÌõÑ Í∏∏Ïù¥ Ï≤¥ÌÅ¨
        text = re.sub(r'\d+_\d+|\[\w+\]|\b\d+\b|\bxxx\b', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        if len(text) > 1:
            if speaker not in active_speakers:
                active_speakers[speaker] = 0
            active_speakers[speaker] += 1
    
    # Î∞úÌôîÏàò Ïàú Ï†ïÎ†¨
    sorted_speakers = dict(sorted(active_speakers.items(), key=lambda x: x[1], reverse=True))
    
    return sorted_speakers

# # ÌÖåÏä§Ìä∏
# speakers = count_utterance_by_speaker("ENNI/SLI/A/413.cha")
# # print(f"üë• ÌôîÏûê {list(speakers.keys())}")
# print("üìä Î∞úÌôî Î∂ÑÌè¨:", speakers)


@dataclass
class Utterance:
    order: int
    speaker: str
    text: str
    clean_text: str

def clean(text: str) -> str:
    """ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ + Î™®Îì† ÌäπÏàòÍ∏∞Ìò∏ Ï†úÍ±∞"""
    
    # 1. ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ 123_456
    text = re.sub(r'\d+_\d+', '', text)
    
    # 2. ÎÇòÎ®∏ÏßÄ ÌäπÏàòÍ∏∞Ìò∏
    text = re.sub(r'\[[^\]]*\]|\b(?:xxx|www|0)\b|[/]|[/=]|&=', '', text)
    text = re.sub(r'[^\w\s\.\,\!\?\-\']', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def extract_utterances(cha_file: str, speakers: List[str]) -> List[Utterance]:
    """
    ÏßÄÏ†ï ÌôîÏûêÎì§Ïùò Î∞úÌôîÎßå ÏàúÏÑúÎåÄÎ°ú Ï∂îÏ∂ú
    
    Args:
        cha_file: '413.cha'
        speakers: ['CHI', 'MOT'] ÎòêÎäî ['CHI', 'EXA']
    
    Returns:
        List[Utterance]: ÏàúÏÑú Ïú†ÏßÄÎêú Î∞úÌôî Î¶¨Ïä§Ìä∏
    """
    content = Path(cha_file).read_text(errors='ignore')
    
    # ÏßÄÏ†ï ÌôîÏûê Ìå®ÌÑ¥ (ÎåÄÏÜåÎ¨∏Ïûê Î¨¥Ïãú)
    speaker_pattern = '|'.join([re.escape(s) for s in speakers])
    pattern = rf'\*({speaker_pattern}):\s*(.*?)(?=\n\*[A-Z][A-Za-z ]+?:|\n%|\n@|\Z)'
    
    matches = list(re.finditer(pattern, content, re.DOTALL | re.I))
    
    utterances = []
    for i, match in enumerate(matches, 1):
        speaker = match.group(1).strip()
        raw_text = match.group(2).strip()
        
        # ÌÅ¥Î¶¨Îãù
        clean_text = clean(raw_text)
        
        if len(clean_text) > 1:  # ÏùòÎØ∏ÏûàÎäî Î∞úÌôîÎßå
            utterances.append(Utterance(
                order=i,
                speaker=speaker,
                text=raw_text,
                clean_text=clean_text
            ))
    
    return utterances


def clip_grads(grads, max_norm):
    total_norm = 0
    for grad in grads:
        total_norm += np.sum(grad ** 2)
    total_norm = np.sqrt(total_norm)

    rate = max_norm / (total_norm + 1e-6)
    if rate < 1:
        for grad in grads:
            grad *= rate

def get_batch(corpus, label, time_size, batch_size, is_random=True, count=0):
    # ÏïÑÎèôÏùò ÏàòÏóêÏÑú batch_sizeÎßåÌÅº ÏàòÎ•º ÎΩëÏùå
    if is_random:
        nums = np.random.choice(len(corpus), size=batch_size, replace=False)
    else:
        nums = np.arange(count*batch_size, (count+1)*batch_size)
    xs_batch = corpus[nums]

    label_batch = label[nums]
    for i, j in enumerate(xs_batch):
        xs_batch[i] = j[:-2]    # ÎßàÏßÄÎßâÏùÄ label
        

    def pad_to(word_list, size):
        length = len(word_list)
        if length >= size:
            return word_list[:size]
        else:
            return word_list + [-1] * (size - length)
    
    xs_batch = np.array([pad_to(list(x), time_size) for x in xs_batch], dtype=int)

    return xs_batch, label_batch



def eval_perplexity(model, corpus, label, batch_size=10, time_size=35):
    print('ÌçºÌîåÎ†âÏÑúÌã∞ ÌèâÍ∞Ä Ï§ë ...')
    corpus_size = len(corpus)
    total_loss, loss_cnt = 0, 0
    max_iters = (corpus_size - 1) // (batch_size * time_size)
    jump = (corpus_size - 1) // batch_size

    for iters in range(max_iters):
        xs = np.zeros((batch_size, time_size), dtype=np.int32)
        ts = np.zeros((batch_size, time_size), dtype=np.int32)
        time_offset = iters * time_size
        offsets = [time_offset + (i * jump) for i in range(batch_size)]
        for t in range(time_size):
            for i, offset in enumerate(offsets):
                xs[i, t] = corpus[(offset + t) % corpus_size]
                ts[i, t] = corpus[(offset + t + 1) % corpus_size]

        try:
            loss = model.forward(xs, ts, train_flg=False)
        except TypeError:
            loss = model.forward(xs, ts)
        total_loss += loss

        sys.stdout.write('\r%d / %d' % (iters, max_iters))
        sys.stdout.flush()

    print('')
    ppl = np.exp(total_loss / max_iters)
    return ppl


def eval_loss(model, corpus, label, batch_size, time_size):
    print('loss, Ï†ïÌôïÎèÑ ÌèâÍ∞Ä Ï§ë ...')
    corpus_size = len(corpus)
    total_loss, total_acc = 0.0, 0.0
    max_iters = max(1, corpus_size // batch_size)

    for iters in range(max_iters):
        xs_dev, label_dev = get_batch(corpus, label, time_size, batch_size, is_random=False, count=iters) 
        try:
            loss, acc = model.forward(xs_dev, label_dev, train_flg=False)
        except TypeError:
            loss = model.forward(xs_dev, label_dev)
        total_loss += loss
        total_acc += acc


    print('')
    avg_loss = total_loss / max_iters
    avg_acc = total_acc / max_iters
    return avg_loss, avg_acc